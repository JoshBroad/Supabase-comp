# Data Lake to SQL

**Turn messy file dumps into structured SQL databases automatically using AI.**

This project is an intelligent "Data Lake to SQL" pipeline. It allows users to upload raw data files (CSV, JSON, XML, Text) into a "Data Lake" (Supabase Storage), and then employs an autonomous AI Agent (powered by LangGraph) to analyze the data, infer a relational schema, resolve entities, and populate a PostgreSQL database with normalized data.

![Data Lake to SQL](https://placehold.co/600x400?text=Data+Lake+to+SQL)

## üöÄ Key Features

- **üìÇ Multi-Format Ingestion**: Upload `CSV`, `JSON`, `XML`, and unstructured `TXT` files directly.
- **üß† AI-Powered ETL**:
  - **Schema Inference**: Automatically detects tables, columns, and data types.
  - **Entity Resolution**: Identifies relationships (foreign keys) between different files (e.g., linking `orders.json` to `customers.csv`).
  - **Data Normalization**: Cleans and structures data for SQL insertion.
- **‚ö° Autonomous Agent**: Uses **LangGraph** to orchestrate a multi-step pipeline:
  1.  **Parse**: Extracts raw data from files.
  2.  **Infer**: Identifies entities and relationships.
  3.  **Generate**: Writes idempotent SQL (DDL & DML).
  4.  **Execute**: Runs SQL against Supabase PostgreSQL.
  5.  **Validate**: Checks for data integrity and schema drift.
- **üìä Real-Time Visualization**:
  - **2D & 3D Knowledge Graphs**: Visualize the evolving database schema and relationships in real-time.
  - **Live Build Timeline**: Watch the agent's progress step-by-step.
- **üõ°Ô∏è Data Drift Detection**: Alerts users when new data doesn't fit the existing schema.

## üèóÔ∏è Architecture

The project consists of three main components:

1.  **Frontend (`data-lake-frontend`)**: A Next.js 14 application for file uploads and real-time visualization.
2.  **Agent (`agent`)**: A Node.js/TypeScript autonomous agent built with **LangGraph** and **LangChain**. It acts as the "Brain", processing data and executing SQL.
3.  **Backend / Database (`data-lake-backend`)**: A **Supabase** project providing:
    - **PostgreSQL Database**: The destination for the structured data.
    - **Storage**: The "Data Lake" for raw file uploads.
    - **Realtime**: Broadcasting agent events to the frontend.
    - **Edge Functions** (Optional): For serverless orchestration.

### Workflow

1.  **Upload**: User uploads files via Frontend -> Supabase Storage.
2.  **Trigger**: Frontend triggers the Agent (via HTTP).
3.  **Process**:
    - Agent downloads files from Supabase Storage.
    - Agent sends file snippets to LLM (OpenRouter) to infer schema.
    - Agent generates SQL `CREATE TABLE` and `INSERT` statements.
4.  **Execute**: Agent executes SQL via Supabase RPC (`exec_sql`).
5.  **Visualize**: Frontend listens to Supabase Realtime channels to update the graph and timeline.

## üõ†Ô∏è Tech Stack

-   **Frontend**: Next.js 14, React, Tailwind CSS, React Flow (2D), React Three Fiber (3D), Lucide Icons.
-   **Agent**: Node.js, LangGraph, LangChain, OpenAI/OpenRouter (LLMs), Zod.
-   **Backend**: Supabase (PostgreSQL, Storage, Realtime, Auth).

## üèÅ Getting Started

### Prerequisites

-   Node.js (v18+)
-   Supabase Account & Project
-   OpenRouter API Key (for LLM access)

### 1. Setup Supabase

1.  Create a new Supabase project.
2.  Run the migrations in `data-lake-backend/supabase/migrations` to set up the control plane tables (`build_sessions`, `build_events`) and the `exec_sql` RPC function.
3.  Create a Storage bucket named `uploads`.

### 2. Configure Environment Variables

**Frontend (`data-lake-frontend/.env.local`)**:
```env
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
NEXT_PUBLIC_BACKEND_BASE_URL=http://localhost:3001 # Points to local agent
```

**Agent (`agent/.env`)**:
```env
OPENROUTER_API_KEY=your_openrouter_key
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key # REQUIRED for admin tasks
PORT=3001
```

### 3. Run the Project

**Start the Agent**:
```bash
cd agent
npm install
npm run dev
```
*The agent will start on port 3001.*

**Start the Frontend**:
```bash
cd data-lake-frontend
npm install
npm run dev
```
*The frontend will start on port 3000.*

### 4. Use the App

1.  Open `http://localhost:3000`.
2.  Upload the sample files (found in `sample-data/`).
3.  Click **"Analyze & Build SQL"**.
4.  Watch the agent build your database in real-time!

## üß© Supabase Features Used

-   **Database**: Core relational data storage.
-   **Storage**: Staging area for raw "Data Lake" files.
-   **Realtime**: Pushing build events (`table_created`, `row_inserted`) to the frontend.
-   **Auth**: Handling anonymous sessions and service-role secure execution.
-   **RPC**: Securely executing dynamic SQL generated by the AI agent.